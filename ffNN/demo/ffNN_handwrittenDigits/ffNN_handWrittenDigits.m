function ffNN = ffNN_handwrittenDigits...
   (numsHid = 100, weightPenaltyTerm = 0, ...
   numIters_perBatch = 3, numEpochs = 30, bestStop = true); 
   
   close all;
   
   % load data
   [trainInput trainTargetOutput trainTargetOutput_labels ...
   validInput validTargetOutput validTargetOutput_labels ...
   testInput testTargetOutput testTargetOutput_labels ...
   imgHeight imgWidth] = load_mNIST_11k;
   
   m = length(trainTargetOutput);
   
   
   % display random sample labels & images
   numDisplaySamples = 10 ^ 2;
   indices_displaySamples = 1 : numDisplaySamples;
   displaySamples_numRowsCols = sqrt(numDisplaySamples);
   displayLabels = trainTargetOutput_labels...
      (indices_displaySamples);
   fprintf('\n%i Sample Labels & Images:\n', ...
      numDisplaySamples);
   reshape(displayLabels, [displaySamples_numRowsCols ...
      displaySamples_numRowsCols])'      
   plot2D_grayImages...
      (permute(trainInput(indices_displaySamples, :, :), ...
      [2 3 1]));   
   
   % create Forward-Feeding Neural Network (FFNN):
   % all layers are Logistic transformation layers
   % with the top layer automatically set as 
   % a 10-way Softmax layer
   ffNN = class_ffNN...
      (inputDimSizes_perCase = 256, ...
      addlLayersNumsNodes = [numsHid 10], ...
      transformFuncs = {'tanh' 'softmax'}, ...
      displayOverview = false, ...
      initWeights_rand = true);   
   
   % reshape data to fit model's architecture
   trainInput_eachIsVector = trainInput(:, :);
   validInput_eachIsVector = validInput(:, :);
   testInput_eachIsVector = testInput(:, :);

   % train FFNN with CONJUGATE GRADIENT
   ffNN = train_conjGrad...
      (ffNN_init___ = ffNN, ...
      dataArgs___ = {trainInput_eachIsVector trainTargetOutput ...
                     validInput_eachIsVector validTargetOutput ...
                     testInput_eachIsVector testTargetOutput}, ...
      targetOutput_isClassIndcsColVec___ = false, ...
      numIters_perBatch___ = numIters_perBatch, ...
      trainNumEpochs___ = numEpochs, ...
      trainBatchSize___ = false, ...
      trainRandShuff___ = false, ...
      trainCostApproxChunk_numBatches = 1, ...
      validCostCalcInterval_numChunks = 3, ...
      weightRegulArgs___ = {{'L2'} [weightPenaltyTerm]}, ...
      connectProbs___ = [1.0], ...
      bestStop___ = bestStop);
      
   % visualize learned 1st-layer weights
   % to see what detected features are like;
   % we will see many features detecting strokes
   % and hooks, which are relevant in distinguishing
   % one digit from another
   weights = ffNN.weights;
   weights_layer1 = weights{1};
   % for visualization purposes, we ignore bias terms
   weights_layer1 = rmBiasElems(weights_layer1);
   % reshape weights for visualization
   weights_layer1 = permute(reshape(weights_layer1', ...
      [numsHid(1) imgHeight imgWidth]), [2 3 1]);
   % visualize weights
   plot2D_grayImages(weights_layer1);
  
   % now analyse Test cases the model still gets wrong
   [mis_predicted_labels mis_targetOutput_labels] = ...
      ffNN_handwrittenDigits_analyseMisclassifs...
      (ffNN, testInput, testTargetOutput_labels);
   numMisclassifs = length(mis_predicted_labels);
   serialNums = (1 : numMisclassifs)';   
   fprintf('Analyse %i Test set mis-classifications:\n', ...
      numMisclassifs);  
   fprintf('   #\n');
   fprintf('   PREDICTED LABEL\n');
   fprintf('   TARGET OUTPUT LABEL\n');   
   [serialNums mis_predicted_labels mis_targetOutput_labels]'
 
endfunction